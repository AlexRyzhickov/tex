
\subsection*{Review 4}

The paper reports on testing the well-known ECCE partial deduction system, in order to compare with a partial deduction for a simplest model variant of Prolog named relation language system being developed by the authors.
It is surely in the scope of the VPT workshop.
The submitted paper is quite good smoothly written. It is rather a kind of a tutorial based on the simplest model relation language than a research paper. I have actually found no research news in this paper. The authors are studying the partial deduction method and try, with a youthful enthusiasm, to dubiously comment some aspects of the simplest version of ("positive") supercompilation, experimenting with simple program samples, and fix their understanding "on the white lists of paper". Nevertheless, studying the program specialization methods is laudable and should be supported.
The introduction, the section "Related works", and the section "References" take five pages from 13 pages used for the whole paper, i.e., these three sections together take more than 38 percents of the paper size.
Unfortunately, the paper contains no transformation example of miniKanren program given in details, thus the reader is not able to follows the details and be sure in soundness of the short comments.
The paper authors do not refer to many well known published papers that of course were read by them and indirectly used in the submitted tutorial. Since the authors do not explicitly formulate their contribution, then I think perhaps the authors understand themselves that the "conservative partial deduction" was studied in published literature and its variants were used in a number of partial deduction systems.
Thus I am forced to estimate this submission only with "a border paper" score.


Abstract:
1) We identify a number of issues, caused by MINIKANREN peculiarities, and describe a novel approach to specialization based on partial deduction and supercompilation.
 --- This reviewer is not sure the described approach is novel, but it is certainly useful for empirical study of partial deduction.
Sec. Introduction
Page 1.
2) --- In the first three paragraphs of this section the authors write their current understanding the basic concepts of the logical programming, assuming the readers are not familiar with the terms of  "relation language", "partial deduction", and "supercompilation". I think the whole account of these paragraphs can be written in a couple sentences.
3) Specialization or partial evaluation [7] is a technique aimed at improving the performance of a program given some information about it beforehand.
 --- Specialization is a wider concept, than the partial evaluation. The last one is just one of the program specialization methods.
4) Typo:
   (i.e. the length of an input list) -$\to$ (e.g. the length of an input list)
5) Control issues in partial deduction of logic programming language PROLOG have been studied before [13]. Unlike Prolog, where atoms in the right-hand side of a clause cannot be arbitrarily reordered without changing the semantics of a program, in MINIKANREN the subgoals of conjunction/disjunction can be freely switched. This opens a yet another possibility for optimization, not taken into account by approaches initially developed in the context of conventional logic programming.
 --- Actually, studying a program specialization in terms of any programming language starts (and goes on) only for a pure functional fragment of the language.

Page 2.
6) In this paper, we study issues which conjunctive partial deduction faces being applied for MINIKANREN. We also describe a novel approach to partial deduction for relational programming, conservative partial deduction. We implemented this approach and compared it with the existing specialization system (ECCE) for several programs. We report here the results of the comparison and discuss why some MINIKANREN programs run slower after specialization.
 --- It is the contribution? https://github.com/JetBrains-Research/OCanren

Sec. RelatedWork

Page 2.
7) Specialization is an attractive technique aimed to improve the performance of a program if some of its arguments are known statically.
 --- It is wrong. Actually the program specialization takes also into account contexts of intermediate computation, for example, composition of function (or relation) calls as your use themselves, trying to permutate calls of the commutative and associative basic logical connectives.
8) The heart of supercompilation-based techniques is driving - a symbolic execution of a program through all possible execution paths.
 --- As a rule, a program cannot be executed "through all possible execution paths", since the number of the paths are usually unbounded and lengths of the most of the paths are usually infinite. Your next sentence "The result of driving is so-called process tree where nodes correspond to configurations which represent computation state." contradicts to the previous one.
9) The two main sources for supercompilation optimizations are aggressive information propagation about variables' values, equalities and disequalities, and precomputing of all deterministic semantic evaluation steps.
 --- It seems to me you mean only deterministic programming languages. Hence, by definition, any "deterministic semantic evaluation step" is deterministic. It should be something like the following. "… precomputing all evaluation actions that can be uniformly done over unknown values of the program variables".
10) Generalization, abstracting away some computed data about the current term, makes folding possible.
 --- As a rule, a (local) generalization is a two-arity function depending on two configurations and, maybe, some nontrivial properties of the configuration automatically discovered by supercompilation before the corresponding generalization invocation starts.
11) The accumulating parameter can be removed by replacing the call with its generalization.
 --- As a rule, the accumulating parameter cannot be removed by generalization; even more the task to recognize the fact that a parameter accumulating is undecidable.
12) There are several ways to ensure process correctness and termination, most-specific generalization (anti-unification) and homeomorphic embedding [6, 10] as a whistle being the most common.
 --- In general, the most-specific generalization is not able prevent nontermination. Actually, it depends on the data set of the corresponding programming language. For example, if the data set is the set of finite strings in a given alphabet, then your statement is wrong. I.e. there are functional programming languages manipulating the character strings, rather than the LISP lists. In the case a programming language is Turing complete, using the most-specific generalization, as a rule, does not lead to any interesting transformations.
13) Other criteria, like the size of the generated program or possible optimizations and execution cost of different language constructions by the target language evaluator, are usually out of consideration [7].
 --- The authors do not understand that any (physical) computing system specified by a given program is unclosed. Hence, its performance efficiency depends on an external environment. The environment comprises, for example, (maybe) a number of sequential compilers transforming the source and residual programs to a code supported by hardware, and the hardware properties.

Page 3.
14) Unfortunately there is neither an automatic procedure to choose what control setting is likely to improve input programs the most nor any informal recommendations on how to choose the best settings.
 --- The sentence above should be rewritten.

Sec. 3 Conservative Partial Deduction
Page 4.
15) A driving process creates a process tree, from which a residual program is later created.
 --- The driving does not include the generalization and folding actions. Unfortunately, the authors being freshmen and a fresh woman do not understand the basic concepts of supercompilation.
16) --- In this section a transformation example demonstrating the "conservative partial deduction" algorithm should given in details. You have spent only 13 pages and have at least two pages for such an example.  There are many problems hidden behind the pseudocode given in Figure 1. Without such an example it is not certainly that the authors understand the problems. For example, what is the strategy looking for the configuration C'? (1) Does the configuration belong to the path rooted at the initial goal and ending at the configuration C'? (2) Or, maybe, does your algorithm look among all generated configurations in the process tree? What is happened when there are at least two such configurations? Following the pseudocode one may guesses that the strategy follows the (1)-st variant. Actually, it is not the best choice. As far as I know the most specialization methods based on the the (2)-nd variant. It is quite natural since the (1)-st strategy leads to generating lager !
 residual programs as compared with the (2)-nd one (or code duplication).
17) "heuristically_select_a_call( r_1 , … , r_n )
 --- Hence, you have no idea on the strategy choosing the next atom. Actually, the set of the strategies controlling the supercompilation process is the key problem hidden by you, since the optimization task per se is undecidable.
18) The substitution computed at each step is also stored in the tree node, although it is not included in the configuration.
 --- What does that mean? Only either an example mentioned above or a precise definition may provide information on the subject.
19) Each other disjunct takes the form of a possibly empty conjunction of relation calls accompanied with a substitution computed from unifications. Any MINIKANREN term can be trivially transformed into the described form. The function normalize in Fig. 1 is assumed to perform term normalization. The code is omitted for brevity.
 --- The code should be presented somewhere in the paper or in an appendix.
20) There are several core ideas behind this algorithm.
 --- Ok. See my remarks above.
21) The first is to select an arbitrary relation to unfold, not necessarily the leftmost which is safe. … heuristically_select_a_call stands for heuristics combination, see section 3.2 for details ...
 --- Let us see below.
22) The second idea is to use a heuristics which decides if unfolding a relation call can lead to discovery of contradictions between conjuncts which in turn leads to restriction of the answer set at specialization-time
 --- It is not a news. There is a huge literature devoted to this subject. For example, a large series of works by A. Pettorossi, M. Proietti and theirs coauthors, unfortunately, not cited by the authors of the paper being reviewed.
 --- Typo (here and many times along the papers): a heuristics -$\to$ a heuristic

Page 5.
23) In this approach, we do not generalize in the same fashion as CPD or supercompilation. Our conjunctions are always split into individual calls and are joined back together only if it is meaningful. If the need for generalization arises, i.e. homeomorphic embedding of conjunctions [3] is detected, then we immediately stop driving this conjunction (line 12). When residualizing such a conjunction, we just generate a conjunction of calls to the input program before specialization.
 --- In order to make convincing the decisions above you have to demonstrate them by means of interesting non-trivial examples.

Sec. 3.1 Unfolding
24) Unfolding too much may create extra unifications, which is by itself a costly operation, or even introduce duplicated computations by propagating the results of unfolding onto neighbouring conjuncts.
 --- Actually none can reason on any efficiency without fixing an efficiency model. As I have noted above the (physical) computing system is unclosed. The efficiency can be seen as kind of energy of the computing system. The task we are interested in is not quite mathematical and sometimes we should look for physical arguments.
25) We believe that the following heuristics provides a reasonable approach to unfolding control.
 --- I do not think that the statement above looks convincing. Do you?
 --- Only a meaningful list of interesting program transformation examples may convince that.


3.2 Less Branching Heuristics

Page 6.
4 Evaluation
26) ECCE is designed for PROLOG programming language and cannot be directly applied for programs, written in MINIKANREN. To be able to compare our approach with ECCE, we converted each input program first to the pure subset of PROLOG, then specialized it with ECCE, and then we converted the result back to MINIKANREN. The conversion to PROLOG is a simple syntactic conversion. In the conversion from PROLOG to MINIKANREN, for each Horn clause a conjunction is generated in which unifications are placed before any relation call.
 --- Actually ECCE can be (almost) directly applied for programs, written in MINIKANREN. You have to implement an interpreter of MINIKANREN in Prolog and specialize the interpreter w.r.t. your MINIKANREN programs. I expect you are not able to specialize a given Prolog interepreter, written in MINIKANREN,  w.r.t. any Prolog program. Are you able?

Page 7.
27) The queries were run on a laptop running Ubuntu 18.04 with quad core Intel Core i5 2.30GHz CPU and 8 GB of RAM. The tables and graphs use the following denotations. Original represents the execution time of a program before any transformations were applied; ECCE - of the program specialized by ECCE with default conjunctive control setting; ConsPD-of the program specialized by our approach. … Figure 3: Execution time of eval$^o$.
 --- I guess you have a series of examples demonstrating a different behaviour of ConsPD as compared with ECCE, but hide such examples. Do not have you?
 --- Have you explored some properties of the operating systems when the examples were specialized by both ConsPD and ECCE? For instance, how much of RAM was occupied, how many program were loaded in the operating system before starting and during the specialization?

Sec. 4.1 Evaluator of Logic Formulas
28) We specialize the eval$^o$ relation to synthesize formulas which evaluate to "true". To do so, we run the specializer for the goal with the last argument fixed to "true", while the first two arguments remain free variables. Depending on the way the eval$^o$ is implemented, different specializers generate significantly different residual programs.
 --- Those are interesting experiments. The specialization results are quite expected.

Sec. 4.1.1 The Order of Relation Calls

Page 8.
29) Knowing that res is "true", we can conclude that in the call and$^o$ v w res variables v and w have to be "true as well. There are three possible options for these variables in the call or$^o$ v w res and one for the call not$^o$. These variables are used in recursive calls of eval$^o$ and thus restrict the result of driving. CPD fails to recognize this, and thus unfolds recursive calls of eval$^o$ applied to fresh variables. It leads to over-unfolding, large residual programs and poor performance.
 --- The problem discussing by you above is a problem of parallel evaluation. Parallel evaluation is natural for the relation programming. It is widely known that the simplest way to solve the problem is to use the breadth-first unfolding.

Sec. 4.1.2 Unfolding of Complex Relations
30) Depending on the way a relation is implemented, it may take a different number of driving steps to reach the point when any useful information is derived through its unfolding.
 --- Here, for the first time in this paper, you use the "driving" term in a correct context, writing on "a different number of driving steps". Given an interpreter (an operation semantics of a programming language), the interpreter has a concept of a step repeated during evaluation (computation) of the programs on their given/fixed input data. The driving is a meta-extension of the step over the parameterized input data. Unfortunately, the data sets of Prolog and relation programming languages themselves include parameters named "free variables". That leads to a confusion.

Page 9.
31) Typo:
  Figure 3: Execution time of evalo -$\to$ Figure 3: Execution time of eval$^o$

Sec. 4.1.4 The Order of Answers
Page 10.

32) We believe that, in general, it is not possible to guarantee the same order of answers after specialization.
 --- That is evidently since, in general, the problem discussed is undecidable.

Sec. 4.2 Typechecker-Term Generator

Page 11.
33) For example, typechecking of the sum of two terms in the hand-written implementation consists of a single conjunction (see Listing 5) while the generated program is far more complicated and also uses a special relation typeEq$^o$ to compare types (see Listing 6).
 --- Here the residual programs should be specialized once again. Is able your model specializer discussed to remove such redundant relation definitions from the residual programs?

Sec. 5 Conclusion
Page 12.
34) We compared this approach with the most sophisticated implementation of conjunctive partial deduction- ECCE partial deduction system-on 6 relations which solve 2 different problems.
 --- Do your really "believe" that such a benchmark size is quite convincing? Don't you?

Thanks for your enthusiasm.
