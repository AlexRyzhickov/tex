
\subsection*{Review 4}

The paper reports on testing the well-known ECCE partial deduction system, in order to compare with a partial deduction for a simplest model variant of Prolog named relation language system being developed by the authors.

\emph{\mk was not developed by the authors. We clarified it in the introduction and added a background subsection on \mk. }

It is surely in the scope of the VPT workshop.
The submitted paper is quite good smoothly written.
It is rather a kind of a tutorial based on the simplest model relation language than a research paper.
I have actually found no research news in this paper.
The authors are studying the partial deduction method and try, with a youthful enthusiasm, to dubiously comment some aspects of the simplest version of ("positive") supercompilation, experimenting with simple program samples, and fix their understanding "on the white lists of paper".
Nevertheless, studying the program specialization methods is laudable and should be supported.
The introduction, the section "Related works", and the section "References" take five pages from 13 pages used for the whole paper, i.e., these three sections together take more than 38 percents of the paper size.
Unfortunately, the paper contains no transformation example of miniKanren program given in details, thus the reader is not able to follows the details and be sure in soundness of the short comments.

\emph{We added an example: see section~\ref{example}.}

The paper authors do not refer to many well known published papers that of course were read by them and indirectly used in the submitted tutorial.

\emph{We added citations to more of the papers read by us, as well as to the papers the reviewers suggested.}

Since the authors do not explicitly formulate their contribution, then I think perhaps the authors understand themselves that the "conservative partial deduction" was studied in published literature and its variants were used in a number of partial deduction systems.

\emph{Links to these papers would be appreciated.}

Thus I am forced to estimate this submission only with "a border paper" score.


Abstract:
\begin{enumerate}
  \item  We identify a number of issues, caused by MINIKANREN peculiarities, and describe a novel approach to specialization based on partial deduction and supercompilation.
  --- This reviewer is not sure the described approach is novel, but it is certainly useful for empirical study of partial deduction.
\end{enumerate}

Sec. Introduction

Page 1.
\begin{enumerate}
  \setcounter{enumi}{1}
  \item{--- In the first three paragraphs of this section the authors write their current understanding the basic concepts of the logical programming, assuming the readers are not familiar with the terms of  "relation language", "partial deduction", and "supercompilation". I think the whole account of these paragraphs can be written in a couple sentences.

  \emph{Our intent was to write a self-contained paper which can be understood by readers with different backgrounds.}}

  \item{Specialization or partial evaluation [7] is a technique aimed at improving the performance of a program given some information about it beforehand.
  --- Specialization is a wider concept, than the partial evaluation. The last one is just one of the program specialization methods.

  \emph{We agree. We rewrote the sentence to avoid confusing the two terms.}}

  \item{Typo:
  (i.e. the length of an input list) $\to$ (e.g. the length of an input list)

  \emph{Fixed.}

  \item{Control issues in partial deduction of logic programming language PROLOG have been studied before [13]. Unlike Prolog, where atoms in the right-hand side of a clause cannot be arbitrarily reordered without changing the semantics of a program, in MINIKANREN the subgoals of conjunction/disjunction can be freely switched. This opens a yet another possibility for optimization, not taken into account by approaches initially developed in the context of conventional logic programming.
  --- Actually, studying a program specialization in terms of any programming language starts (and goes on) only for a pure functional fragment of the language.
  }
  }

\end{enumerate}

Page 2.

\begin{enumerate}
  \setcounter{enumi}{5}
  \item In this paper, we study issues which conjunctive partial deduction faces being applied for MINIKANREN. We also describe a novel approach to partial deduction for relational programming, conservative partial deduction. We implemented this approach and compared it with the existing specialization system (ECCE) for several programs. We report here the results of the comparison and discuss why some MINIKANREN programs run slower after specialization.
  --- It is the contribution? https://github.com/JetBrains-Research/OCanren

  \emph{No, this is not the contribution. However, we used this \mk implementation to run the programs mentioned in the paper. The contributions of the paper are listed in the last paragraph of the Introduction.}
\end{enumerate}

Sec. RelatedWork

Page 2.

\begin{enumerate}
  \setcounter{enumi}{6}
  \item {Specialization is an attractive technique aimed to improve the performance of a program if some of its arguments are known statically.

  --- It is wrong. Actually the program specialization takes also into account contexts of intermediate computation, for example, composition of function (or relation) calls as your use themselves, trying to permutate calls of the commutative and associative basic logical connectives.

  \emph{The sentence was rewritten to reflect this wider context.}
  }

  \item{
    The heart of supercompilation-based techniques is driving - a symbolic execution of a program through all possible execution paths.

    --- As a rule, a program cannot be executed ``through all possible execution paths'', since the number of the paths are usually unbounded and lengths of the most of the paths are usually infinite. Your next sentence ``The result of driving is so-called process tree where nodes correspond to configurations which represent computation state.'' contradicts to the previous one.

    \emph{A process tree can be infinite, thus representing an unbound number of the paths.
    Infinite data structures are not something particularly hard to deal with both in code and in theoretical reasoning.}

    \item {
      The two main sources for supercompilation optimizations are aggressive information propagation about variables' values, equalities and disequalities, and precomputing of all deterministic semantic evaluation steps.

      --- It seems to me you mean only deterministic programming languages. Hence, by definition, any ``deterministic semantic evaluation step'' is deterministic. It should be something like the following. ``\dots precomputing all evaluation actions that can be uniformly done over unknown values of the program variables''.

      \emph{By deterministic semantic step we mean the situation in which there is a node in the process tree which has only one branch --- also known as transient nodes.}
    }

    \item{
      Generalization, abstracting away some computed data about the current term, makes folding possible.

      --- As a rule, a (local) generalization is a two-arity function depending on two configurations and, maybe, some nontrivial properties of the configuration automatically discovered by supercompilation before the corresponding generalization invocation starts.

      % \emph{We do not see any actionable items in this comment.}
    }

    \item{
      The accumulating parameter can be removed by replacing the call with its generalization.

      --- As a rule, the accumulating parameter cannot be removed by generalization; even more the task to recognize the fact that a parameter accumulating is undecidable.

      \emph{We are not stating that generalization can identify any accumulating parameter and remove it. Sometimes accumulating parameters can be identified, for example, by homeomorphic embedding, and then removed by most specific generalization.}
    }

    \item{
      There are several ways to ensure process correctness and termination, most-specific generalization (anti-unification) and homeomorphic embedding [6, 10] as a whistle being the most common.

      --- In general, the most-specific generalization is not able prevent nontermination. Actually, it depends on the data set of the corresponding programming language. For example, if the data set is the set of finite strings in a given alphabet, then your statement is wrong. I.e. there are functional programming languages manipulating the character strings, rather than the LISP lists. In the case a programming language is Turing complete, using the most-specific generalization, as a rule, does not lead to any interesting transformations.

      \emph{We were referring to the resutls on the convergence of transfromers from the paper~\cite{sorensen1998convergence}.}
    }

    \item{Other criteria, like the size of the generated program or possible optimizations and execution cost of different language constructions by the target language evaluator, are usually out of consideration [7].

    --- The authors do not understand that any (physical) computing system specified by a given program is unclosed. Hence, its performance efficiency depends on an external environment. The environment comprises, for example, (maybe) a number of sequential compilers transforming the source and residual programs to a code supported by hardware, and the hardware properties.

    \emph{We understand this. However abstracting away from such details is a common practice in estimating how well some transformation works.}
    }

  }
\end{enumerate}

Page 3.

\begin{enumerate}
  \setcounter{enumi}{13}
  \item { Unfortunately there is neither an automatic procedure to choose what control setting is likely to improve input programs the most nor any informal recommendations on how to choose the best settings.

  --- The sentence above should be rewritten.

  \emph{The sentence was rewritten.}
  }
\end{enumerate}

Sec. 3 Conservative Partial Deduction

Page 4.

\begin{enumerate}
  \setcounter{enumi}{14}
  \item { A driving process creates a process tree, from which a residual program is later created.

  --- The driving does not include the generalization and folding actions. Unfortunately, the authors being freshmen and a fresh woman do not understand the basic concepts of supercompilation.

  \emph{We mentioned generalization and folding in the sentence.}}

  \item { --- In this section a transformation example demonstrating the ``conservative partial deduction'' algorithm should given in details. You have spent only 13 pages and have at least two pages for such an example.  There are many problems hidden behind the pseudocode given in Figure 1. Without such an example it is not certainly that the authors understand the problems. For example, what is the strategy looking for the configuration C'? (1) Does the configuration belong to the path rooted at the initial goal and ending at the configuration C'? (2) Or, maybe, does your algorithm look among all generated configurations in the process tree? What is happened when there are at least two such configurations? Following the pseudocode one may guesses that the strategy follows the (1)-st variant. Actually, it is not the best choice. As far as I know the most specialization methods based on the the (2)-nd variant. It is quite natural since the (1)-st strategy leads to generating lager !
  residual programs as compared with the (2)-nd one (or code duplication).

  \emph{Currently, the less branching heuristics guides the choice of the suitable configuration. It works in the following manner. First it searches for the a static call within a conjunction. If there is no static calls, it then tries to find a deterministic call. If there are none, then it searches for the call which branchess less, than it does when all arguments are distinct free variables. It does select the leftmost suitable call, but only in the order described. If the leftmost call is less-branching, but the rightmost call is static, then the rightmost call will be selected and then relaced in the conjunction with the result of its unfolding. }
  }

  \item{\verb!heuristically_select_a_call( r_1 , ... , r_n )!

  --- Hence, you have no idea on the strategy choosing the next atom. Actually, the set of the strategies controlling the supercompilation process is the key problem hidden by you, since the optimization task per se is undecidable.

  \emph{The strategy of choosing the next atom is exactly what is described in the section~\ref{sec:heurictic}}
  }

  \item{
    The substitution computed at each step is also stored in the tree node, although it is not included in the configuration.

    --- What does that mean? Only either an example mentioned above or a precise definition may provide information on the subject.

    \emph{It means that the substitution is stored in the process tree, since it is later used in residualization. Substitutions are ignored by the whistle: only goals in configurations are checked for renaming and embedding. We clarified it in the paper.}
  }

  \item{
    Each other disjunct takes the form of a possibly empty conjunction of relation calls accompanied with a substitution computed from unifications. Any MINIKANREN term can be trivially transformed into the described form. The function normalize in Fig. 1 is assumed to perform term normalization. The code is omitted for brevity.

    --- The code should be presented somewhere in the paper or in an appendix.

    \emph{The implementation of the approach can be found on the github, the link is added into the paper.}
  }

  \item{
    There are several core ideas behind this algorithm.

    --- Ok. See my remarks above.

    % \emph{We do not see any actionable items in this comment.}
  }

  \item{
    The first is to select an arbitrary relation to unfold, not necessarily the leftmost which is safe. … heuristically_select_a_call stands for heuristics combination, see section 3.2 for details ...

    --- Let us see below.

    % \emph{We do not see any actionable items in this comment.}
  }

  \item{
    The second idea is to use a heuristics which decides if unfolding a relation call can lead to discovery of contradictions between conjuncts which in turn leads to restriction of the answer set at specialization-time

    --- It is not a news. There is a huge literature devoted to this subject. For example, a large series of works by A. Pettorossi, M. Proietti and theirs coauthors, unfortunately, not cited by the authors of the paper being reviewed.

    \emph{The list of these works would be greatly appreciated.}
  }

  \item{ --- Typo (here and many times along the papers): a heuristics -$\to$ a heuristic

  \emph{Fixed.}
  }

\end{enumerate}

Page 5.

\begin{enumerate}
  \setcounter{enumi}{23}
  \item {
    In this approach, we do not generalize in the same fashion as CPD or supercompilation. Our conjunctions are always split into individual calls and are joined back together only if it is meaningful. If the need for generalization arises, i.e. homeomorphic embedding of conjunctions [3] is detected, then we immediately stop driving this conjunction (line 12). When residualizing such a conjunction, we just generate a conjunction of calls to the input program before specialization.

    --- In order to make convincing the decisions above you have to demonstrate them by means of interesting non-trivial examples.

    \emph{We believe the examples in the evaluation are  interesting, non-trivial, and demonstrate the benefits of the decisions made. The fact that we only do splitting in generalization and do not perform most-specific generalization of the terms embedded was an attempt to reduce the complexity of the system so that it can be easier to comprehend. This limitation is not set in stone is not hard to remove. }

  }
\end{enumerate}

Sec. 3.1 Unfolding

\begin{enumerate}
  \setcounter{enumi}{24}
  \item {Unfolding too much may create extra unifications, which is by itself a costly operation, or even introduce duplicated computations by propagating the results of unfolding onto neighbouring conjuncts.

  --- Actually none can reason on any efficiency without fixing an efficiency model. As I have noted above the (physical) computing system is unclosed. The efficiency can be seen as kind of energy of the computing system. The task we are interested in is not quite mathematical and sometimes we should look for physical arguments.

  % \emph{We do not see any actionable items in this comment.}
  }

  \item {
    We believe that the following heuristics provides a reasonable approach to unfolding control.

 --- I do not think that the statement above looks convincing. Do you?

 --- Only a meaningful list of interesting program transformation examples may convince that.

 \emph{We believe the examples in the evaluation are interesting, non-trivial, and demonstrate the benefitsof the decisions made.}
  }
\end{enumerate}

3.2 Less Branching Heuristics

Page 6.

4 Evaluation

\begin{enumerate}
  \setcounter{enumi}{26}
  \item {
    ECCE is designed for PROLOG programming language and cannot be directly applied for programs, written in MINIKANREN. To be able to compare our approach with ECCE, we converted each input program first to the pure subset of PROLOG, then specialized it with ECCE, and then we converted the result back to MINIKANREN. The conversion to PROLOG is a simple syntactic conversion. In the conversion from PROLOG to MINIKANREN, for each Horn clause a conjunction is generated in which unifications are placed before any relation call.

    --- Actually ECCE can be (almost) directly applied for programs, written in MINIKANREN. You have to implement an interpreter of MINIKANREN in Prolog and specialize the interpreter w.r.t. your MINIKANREN programs. I expect you are not able to specialize a given Prolog interepreter, written in MINIKANREN,  w.r.t. any Prolog program. Are you able?

    \emph{This would definitely be a fun experiment to run. We did not do it, if that is what you are asking for.}
  }
\end{enumerate}

Page 7.

\begin{enumerate}
  \setcounter{enumi}{27}

  \item {The queries were run on a laptop running Ubuntu 18.04 with quad core Intel Core i5 2.30GHz CPU and 8 GB of RAM. The tables and graphs use the following denotations. Original represents the execution time of a program before any transformations were applied; ECCE - of the program specialized by ECCE with default conjunctive control setting; ConsPD-of the program specialized by our approach. … Figure 3: Execution time of eval$^o$.

  --- I guess you have a series of examples demonstrating a different behaviour of ConsPD as compared with ECCE, but hide such examples. Do not have you?

  \emph{ConsPD currently fails to perform deforestation and tupling on some programs which ECCE successfully transforms. We added a section~\ref{discussion} which discusses it. Battling this shortcomming is an ongoing project. }

  --- Have you explored some properties of the operating systems when the examples were specialized by both ConsPD and ECCE? For instance, how much of RAM was occupied, how many program were loaded in the operating system before starting and during the specialization?

  \emph{We did not, since currently we are only interested in the execution time.}
  }

\end{enumerate}

Sec. 4.1 Evaluator of Logic Formulas

\begin{enumerate}
  \setcounter{enumi}{28}
  \item {
    We specialize the eval$^o$ relation to synthesize formulas which evaluate to "true". To do so, we run the specializer for the goal with the last argument fixed to "true", while the first two arguments remain free variables. Depending on the way the eval$^o$ is implemented, different specializers generate significantly different residual programs.

    --- Those are interesting experiments. The specialization results are quite expected.

    \emph{Thank you.}
  }
\end{enumerate}

Sec. 4.1.1 The Order of Relation Calls

Page 8.

\begin{enumerate}
  \setcounter{enumi}{29}
  \item {
  Knowing that res is ``true'', we can conclude that in the call and$^o$ v w res variables v and w have to be ``true'' as well. There are three possible options for these variables in the call or$^o$ v w res and one for the call not$^o$. These variables are used in recursive calls of eval$^o$ and thus restrict the result of driving. CPD fails to recognize this, and thus unfolds recursive calls of eval$^o$ applied to fresh variables. It leads to over-unfolding, large residual programs and poor performance.

  --- The problem discussing by you above is a problem of parallel evaluation. Parallel evaluation is natural for the relation programming. It is widely known that the simplest way to solve the problem is to use the breadth-first unfolding.

  \emph{If we understand what you meant correctly, then breadth-first unfolding would not be enough since there is often a need to synchronize how much individual relation calls are unfolded to find contradictions. }
  }

\end{enumerate}

Sec. 4.1.2 Unfolding of Complex Relations

\begin{enumerate}
  \setcounter{enumi}{30}
  \item Depending on the way a relation is implemented, it may take a different number of driving steps to reach the point when any useful information is derived through its unfolding.

  --- Here, for the first time in this paper, you use the ``driving'' term in a correct context, writing on ``a different number of driving steps''. Given an interpreter (an operation semantics of a programming language), the interpreter has a concept of a step repeated during evaluation (computation) of the programs on their given/fixed input data. The driving is a meta-extension of the step over the parameterized input data. Unfortunately, the data sets of Prolog and relation programming languages themselves include parameters named ``free variables''. That leads to a confusion.

  % \emph{We do not see any actionable items in this comment.}

\end{enumerate}

Page 9.

\begin{enumerate}
  \setcounter{enumi}{31}
  \item {Typo:
  Figure 3: Execution time of evalo $\to$ Figure 3: Execution time of eval$^o$

  \emph{Fixed.}
  }
\end{enumerate}

Sec. 4.1.4 The Order of Answers

Page 10.

\begin{enumerate}
  \setcounter{enumi}{32}
  \item We believe that, in general, it is not possible to guarantee the same order of answers after specialization.

  --- That is evidently since, in general, the problem discussed is undecidable.

  \emph{Rephrased}
\end{enumerate}

Sec. 4.2 Typechecker-Term Generator

Page 11.

\begin{enumerate}
  \setcounter{enumi}{33}
  \item For example, typechecking of the sum of two terms in the hand-written implementation consists of a single conjunction (see Listing 5) while the generated program is far more complicated and also uses a special relation typeEq$^o$ to compare types (see Listing 6).

  --- Here the residual programs should be specialized once again. Is able your model specializer discussed to remove such redundant relation definitions from the residual programs?

  \emph{Our specializer is able to remove many redundant relation definitions. We did not attempt running the specializer multiple times. }
\end{enumerate}

Sec. 5 Conclusion

Page 12.

\begin{enumerate}
  \setcounter{enumi}{34}
  \item We compared this approach with the most sophisticated implementation of conjunctive partial deduction- ECCE partial deduction system-on 6 relations which solve 2 different problems.

  --- Do your really ``believe'' that such a benchmark size is quite convincing? Don't you?

  \emph{We agree that the evaluation is not convincing as a proper benchmark set. Our goal was to illustrate address the issues which arise often in relational interpreters. Adding more non-trivial programs into the evaluation is an ongoing project.}
\end{enumerate}

Thanks for your enthusiasm.

% \emph{Thank you for taking the time and effort to write such a detailed review!}
